{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing Charging Sessions Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import important libraries needed for the following tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the necessary dataset (charging sessions of EVs) with the data of charging garages in Los Angeles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.read_csv(\"data/charging_sessions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting a first sight of the data by displaying the first 5 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>connectionTime</th>\n",
       "      <th>disconnectTime</th>\n",
       "      <th>doneChargingTime</th>\n",
       "      <th>kWhDelivered</th>\n",
       "      <th>sessionID</th>\n",
       "      <th>siteID</th>\n",
       "      <th>spaceID</th>\n",
       "      <th>stationID</th>\n",
       "      <th>timezone</th>\n",
       "      <th>userID</th>\n",
       "      <th>userInputs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5e23b149f9af8b5fe4b973cf</td>\n",
       "      <td>2020-01-02 13:08:54+00:00</td>\n",
       "      <td>2020-01-02 19:11:15+00:00</td>\n",
       "      <td>2020-01-02 17:31:35+00:00</td>\n",
       "      <td>25.016</td>\n",
       "      <td>1_1_179_810_2020-01-02 13:08:53.870034</td>\n",
       "      <td>1</td>\n",
       "      <td>AG-3F30</td>\n",
       "      <td>1-1-179-810</td>\n",
       "      <td>America/Los_Angeles</td>\n",
       "      <td>194.0</td>\n",
       "      <td>[{'WhPerMile': 250, 'kWhRequested': 25.0, 'mil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5e23b149f9af8b5fe4b973d0</td>\n",
       "      <td>2020-01-02 13:36:50+00:00</td>\n",
       "      <td>2020-01-02 22:38:21+00:00</td>\n",
       "      <td>2020-01-02 20:18:05+00:00</td>\n",
       "      <td>33.097</td>\n",
       "      <td>1_1_193_825_2020-01-02 13:36:49.599853</td>\n",
       "      <td>1</td>\n",
       "      <td>AG-1F01</td>\n",
       "      <td>1-1-193-825</td>\n",
       "      <td>America/Los_Angeles</td>\n",
       "      <td>4275.0</td>\n",
       "      <td>[{'WhPerMile': 280, 'kWhRequested': 70.0, 'mil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5e23b149f9af8b5fe4b973d1</td>\n",
       "      <td>2020-01-02 13:56:35+00:00</td>\n",
       "      <td>2020-01-03 00:39:22+00:00</td>\n",
       "      <td>2020-01-02 16:35:06+00:00</td>\n",
       "      <td>6.521</td>\n",
       "      <td>1_1_193_829_2020-01-02 13:56:35.214993</td>\n",
       "      <td>1</td>\n",
       "      <td>AG-1F03</td>\n",
       "      <td>1-1-193-829</td>\n",
       "      <td>America/Los_Angeles</td>\n",
       "      <td>344.0</td>\n",
       "      <td>[{'WhPerMile': 400, 'kWhRequested': 8.0, 'mile...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5e23b149f9af8b5fe4b973d2</td>\n",
       "      <td>2020-01-02 13:59:58+00:00</td>\n",
       "      <td>2020-01-02 16:38:39+00:00</td>\n",
       "      <td>2020-01-02 15:18:45+00:00</td>\n",
       "      <td>2.355</td>\n",
       "      <td>1_1_193_820_2020-01-02 13:59:58.309319</td>\n",
       "      <td>1</td>\n",
       "      <td>AG-1F04</td>\n",
       "      <td>1-1-193-820</td>\n",
       "      <td>America/Los_Angeles</td>\n",
       "      <td>1117.0</td>\n",
       "      <td>[{'WhPerMile': 400, 'kWhRequested': 8.0, 'mile...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5e23b149f9af8b5fe4b973d3</td>\n",
       "      <td>2020-01-02 14:00:01+00:00</td>\n",
       "      <td>2020-01-02 22:08:40+00:00</td>\n",
       "      <td>2020-01-02 18:17:30+00:00</td>\n",
       "      <td>13.375</td>\n",
       "      <td>1_1_193_819_2020-01-02 14:00:00.779967</td>\n",
       "      <td>1</td>\n",
       "      <td>AG-1F06</td>\n",
       "      <td>1-1-193-819</td>\n",
       "      <td>America/Los_Angeles</td>\n",
       "      <td>334.0</td>\n",
       "      <td>[{'WhPerMile': 400, 'kWhRequested': 16.0, 'mil...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                        id             connectionTime  \\\n",
       "0           0  5e23b149f9af8b5fe4b973cf  2020-01-02 13:08:54+00:00   \n",
       "1           1  5e23b149f9af8b5fe4b973d0  2020-01-02 13:36:50+00:00   \n",
       "2           2  5e23b149f9af8b5fe4b973d1  2020-01-02 13:56:35+00:00   \n",
       "3           3  5e23b149f9af8b5fe4b973d2  2020-01-02 13:59:58+00:00   \n",
       "4           4  5e23b149f9af8b5fe4b973d3  2020-01-02 14:00:01+00:00   \n",
       "\n",
       "              disconnectTime           doneChargingTime  kWhDelivered  \\\n",
       "0  2020-01-02 19:11:15+00:00  2020-01-02 17:31:35+00:00        25.016   \n",
       "1  2020-01-02 22:38:21+00:00  2020-01-02 20:18:05+00:00        33.097   \n",
       "2  2020-01-03 00:39:22+00:00  2020-01-02 16:35:06+00:00         6.521   \n",
       "3  2020-01-02 16:38:39+00:00  2020-01-02 15:18:45+00:00         2.355   \n",
       "4  2020-01-02 22:08:40+00:00  2020-01-02 18:17:30+00:00        13.375   \n",
       "\n",
       "                                sessionID  siteID  spaceID    stationID  \\\n",
       "0  1_1_179_810_2020-01-02 13:08:53.870034       1  AG-3F30  1-1-179-810   \n",
       "1  1_1_193_825_2020-01-02 13:36:49.599853       1  AG-1F01  1-1-193-825   \n",
       "2  1_1_193_829_2020-01-02 13:56:35.214993       1  AG-1F03  1-1-193-829   \n",
       "3  1_1_193_820_2020-01-02 13:59:58.309319       1  AG-1F04  1-1-193-820   \n",
       "4  1_1_193_819_2020-01-02 14:00:00.779967       1  AG-1F06  1-1-193-819   \n",
       "\n",
       "              timezone  userID  \\\n",
       "0  America/Los_Angeles   194.0   \n",
       "1  America/Los_Angeles  4275.0   \n",
       "2  America/Los_Angeles   344.0   \n",
       "3  America/Los_Angeles  1117.0   \n",
       "4  America/Los_Angeles   334.0   \n",
       "\n",
       "                                          userInputs  \n",
       "0  [{'WhPerMile': 250, 'kWhRequested': 25.0, 'mil...  \n",
       "1  [{'WhPerMile': 280, 'kWhRequested': 70.0, 'mil...  \n",
       "2  [{'WhPerMile': 400, 'kWhRequested': 8.0, 'mile...  \n",
       "3  [{'WhPerMile': 400, 'kWhRequested': 8.0, 'mile...  \n",
       "4  [{'WhPerMile': 400, 'kWhRequested': 16.0, 'mil...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows: 66450, columns: 13\n"
     ]
    }
   ],
   "source": [
    "num_rows, num_columns = data_raw.shape\n",
    "print(f\"rows: {num_rows}, columns: {num_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data in the dataset contains 66.450 observation over 13 different variables. Next, we drop duplicates and have closer look at the different datatypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'connectionTime', 'disconnectTime', 'doneChargingTime', 'kWhDelivered',\n",
    "    'sessionID', 'siteID', 'spaceID', 'stationID', 'timezone', 'userID', 'userInputs'\n",
    "]\n",
    "data_unique = data_raw.drop_duplicates(subset=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Row Datetype\n",
      "0         Unnamed: 0    int64\n",
      "1                 id   object\n",
      "2     connectionTime   object\n",
      "3     disconnectTime   object\n",
      "4   doneChargingTime   object\n",
      "5       kWhDelivered  float64\n",
      "6          sessionID   object\n",
      "7             siteID    int64\n",
      "8            spaceID   object\n",
      "9          stationID   object\n",
      "10          timezone   object\n",
      "11            userID  float64\n",
      "12        userInputs   object\n"
     ]
    }
   ],
   "source": [
    "dtypes_data = data_unique.dtypes.reset_index()\n",
    "dtypes_data.columns = ['Row', 'Datetype']\n",
    "print(dtypes_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert all the data into corresponding data types so that we can use them in the further analysis. \n",
    "The time-related data is given in the UTC time zone, but the garages are in Los Angeles. In order to obtain accurate information about the timestamps, the data must therefore be converted to the corresponding time zone (LosAngeles).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marlo\\AppData\\Local\\Temp\\ipykernel_34600\\2943999569.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_unique['id'] = data_unique['id'].astype(\"string\")\n",
      "C:\\Users\\marlo\\AppData\\Local\\Temp\\ipykernel_34600\\2943999569.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_unique['connectionTime'] = pd.to_datetime(data_unique['connectionTime'], utc=True).dt.tz_convert('America/Los_Angeles').dt.tz_localize(None)\n",
      "C:\\Users\\marlo\\AppData\\Local\\Temp\\ipykernel_34600\\2943999569.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_unique['disconnectTime'] = pd.to_datetime(data_unique['disconnectTime'], utc=True).dt.tz_convert('America/Los_Angeles').dt.tz_localize(None)\n",
      "C:\\Users\\marlo\\AppData\\Local\\Temp\\ipykernel_34600\\2943999569.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_unique['doneChargingTime'] = pd.to_datetime(data_unique['doneChargingTime'], utc=True).dt.tz_convert('America/Los_Angeles').dt.tz_localize(None)\n",
      "C:\\Users\\marlo\\AppData\\Local\\Temp\\ipykernel_34600\\2943999569.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_unique['sessionID'] = data_unique['sessionID'].astype(\"string\")\n",
      "C:\\Users\\marlo\\AppData\\Local\\Temp\\ipykernel_34600\\2943999569.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_unique['siteID'] = data_unique['siteID'].astype(int)\n",
      "C:\\Users\\marlo\\AppData\\Local\\Temp\\ipykernel_34600\\2943999569.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_unique['spaceID'] = data_unique['spaceID'].astype(\"string\")\n",
      "C:\\Users\\marlo\\AppData\\Local\\Temp\\ipykernel_34600\\2943999569.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_unique['stationID'] = data_unique['stationID'].astype(\"string\")\n",
      "C:\\Users\\marlo\\AppData\\Local\\Temp\\ipykernel_34600\\2943999569.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_unique['timezone'] = data_unique['timezone'].astype(\"string\")\n",
      "C:\\Users\\marlo\\AppData\\Local\\Temp\\ipykernel_34600\\2943999569.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_unique['userID'] = data_unique['userID'].astype(\"string\")\n",
      "C:\\Users\\marlo\\AppData\\Local\\Temp\\ipykernel_34600\\2943999569.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_unique['sessionID'] = data_unique['sessionID'].astype(\"string\")\n",
      "C:\\Users\\marlo\\AppData\\Local\\Temp\\ipykernel_34600\\2943999569.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_unique['userInputs'] = data_unique['userInputs'].apply(lambda x: list(x) if isinstance(x, list) else [x])\n"
     ]
    }
   ],
   "source": [
    "data_unique['id'] = data_unique['id'].astype(\"string\")\n",
    "data_unique['connectionTime'] = pd.to_datetime(data_unique['connectionTime'], utc=True).dt.tz_convert('America/Los_Angeles').dt.tz_localize(None)\n",
    "data_unique['disconnectTime'] = pd.to_datetime(data_unique['disconnectTime'], utc=True).dt.tz_convert('America/Los_Angeles').dt.tz_localize(None)\n",
    "data_unique['doneChargingTime'] = pd.to_datetime(data_unique['doneChargingTime'], utc=True).dt.tz_convert('America/Los_Angeles').dt.tz_localize(None)\n",
    "data_unique['sessionID'] = data_unique['sessionID'].astype(\"string\")\n",
    "data_unique['siteID'] = data_unique['siteID'].astype(int)\n",
    "data_unique['spaceID'] = data_unique['spaceID'].astype(\"string\")\n",
    "data_unique['stationID'] = data_unique['stationID'].astype(\"string\")\n",
    "data_unique['timezone'] = data_unique['timezone'].astype(\"string\")\n",
    "data_unique['userID'] = data_unique['userID'].astype(\"string\")\n",
    "data_unique['sessionID'] = data_unique['sessionID'].astype(\"string\")\n",
    "data_unique['userInputs'] = data_unique['userInputs'].apply(lambda x: list(x) if isinstance(x, list) else [x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `unnamed` column is presumably an index from the riginal DataFrame and gets transferred into a column when reading the CSV-file. Since our data contains a unique ID for each entry, the unnamed-numbering is not needed and can be deleted. We also delete the id of the respective session (`sessionID`) and loading processes (`id`) because we consider these to be irrelevant for the analysis. every user has an id and so does every station, so that the individual loading processes can be assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connectionTime      datetime64[ns]\n",
      "disconnectTime      datetime64[ns]\n",
      "doneChargingTime    datetime64[ns]\n",
      "kWhDelivered               float64\n",
      "siteID                       int32\n",
      "spaceID             string[python]\n",
      "stationID           string[python]\n",
      "timezone            string[python]\n",
      "userID              string[python]\n",
      "userInputs                  object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "data_unique = data_unique.drop('Unnamed: 0', axis=1)\n",
    "data_unique = data_unique.drop('sessionID', axis=1)\n",
    "data_unique = data_unique.drop('id', axis=1)\n",
    "\n",
    "# create Table with missing values in doneChargingTime\n",
    "miss_doneChargingTime = data_unique[data_unique['doneChargingTime'].isna()]\n",
    "\n",
    "print(data_unique.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 65037 entries, 0 to 65036\n",
      "Data columns (total 10 columns):\n",
      " #   Column            Non-Null Count  Dtype         \n",
      "---  ------            --------------  -----         \n",
      " 0   connectionTime    65037 non-null  datetime64[ns]\n",
      " 1   disconnectTime    65037 non-null  datetime64[ns]\n",
      " 2   doneChargingTime  60950 non-null  datetime64[ns]\n",
      " 3   kWhDelivered      65037 non-null  float64       \n",
      " 4   siteID            65037 non-null  int64         \n",
      " 5   spaceID           65037 non-null  string        \n",
      " 6   stationID         65037 non-null  string        \n",
      " 7   timezone          65037 non-null  string        \n",
      " 8   userID            47822 non-null  string        \n",
      " 9   userInputs        65037 non-null  object        \n",
      "dtypes: datetime64[ns](3), float64(1), int64(1), object(1), string(4)\n",
      "memory usage: 5.5+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(data_unique.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the dataframe, you can see that `doneChargingTime` and `userID` have missing values.  \n",
    "\n",
    "The missing values in userID can be explained by the fact that not every garage user is registered; therefore, some accounts are naturally absent. However, the missing entries in `doneChargingTime` appear unusual.\n",
    "\n",
    "Upon closer analysis of the missing entries for the charging time (`miss_doneChargingTime`), we found that all of them have normal values for the attribute `kWhDelivered`, indicating that current flow was indeed present. Removing this data from the dataframe would imply that the charging stations were free at those times, which is inaccurate and would skew predictions. Therefore, we decided to retain this data for now and, if necessary, exclude it during model training and predictions about parking space utilization after charging is complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset in two different. One for the station with siteID 1 and one for the station with the siteID 2 for further analysis\n",
    "data_1 = data_unique[data_unique['siteID']==1]\n",
    "data_2 = data_unique[data_unique['siteID']==2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing Weather Burbank Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weather dataset is also imported and duplicates are removed. Although there are none, we have carried out this step to be on the safe side.  \n",
    "The data was converted to the corresponding data types and the time was adjusted to the los-angeles time zone again.  \n",
    "There is some missing data in this data set for the `temperature` and the `cloud_cover`. However, these are difficult to reproduce and are therefore deleted from the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city                       string[python]\n",
      "timestamp                  datetime64[ns]\n",
      "temperature                       float64\n",
      "cloud_cover                       float64\n",
      "cloud_cover_description    string[python]\n",
      "pressure                          float64\n",
      "windspeed                         float64\n",
      "precipitation                     float64\n",
      "felt_temperature                  float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "weather_raw=pd.read_csv(\"data/weather_burbank_airport.csv\")\n",
    "\n",
    "# delete duplicates\n",
    "columns = [\n",
    "    'city', 'timestamp', 'temperature', 'cloud_cover',\n",
    "    'cloud_cover_description', 'pressure', 'windspeed', 'precipitation', 'felt_temperature'\n",
    "]\n",
    "weather_unique = weather_raw.drop_duplicates(subset=columns)\n",
    "\n",
    "#Transform to correct Data Types\n",
    "weather_unique['city']=weather_raw['city'].astype('string')\n",
    "weather_unique['timestamp'] = pd.to_datetime(weather_raw['timestamp'], utc=True).dt.tz_convert('America/Los_Angeles').dt.tz_localize(None)\n",
    "weather_unique['cloud_cover_description']=weather_raw['cloud_cover_description'].astype('string')\n",
    "\n",
    "# drop with missing values\n",
    "weather_unique = weather_unique.dropna(subset=['temperature'])\n",
    "weather_unique = weather_unique.dropna(subset=['cloud_cover'])\n",
    "\n",
    "print(weather_unique.dtypes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
